---
title: "Early detection of changes in COVID-19 incidence using NHS pathways data"
author: "Thibaut Jombart, ... for CMMID"
date: "`r format(Sys.time(), '%A %d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: zenburn
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_collapse: no
    toc_depth: 4
    toc_float: yes
    css: !expr here::here('css', 'style.css')
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      collapse = TRUE,
                      dpi = 300,
                      fig.height = 4,
                      fig.width = 8,
                      warning = FALSE,
                      message = FALSE,
                      fig.path = "figures/nhs_paths/",
                      dev = c("pdf", "png"))
```



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->

# Data preparation {.tabset .tabset-fade .tabset-pills}

## Outline


* **Load scripts**: loads libraries and useful scripts used in the analyses; all
`.R` files contained in `scripts` at the root of the factory are automatically
loaded

* **Load data**: imports datasets, and may contain some *ad hoc* changes to the
data such as specific data cleaning (not used in other reports), new variables
used in the analyses, etc.



## Load packages

```{r libraries}

library(reportfactory)
library(here)
library(rio) 
library(tidyverse)
library(incidence)
library(distcrete)
library(epitrix)
library(earlyR)
library(projections)
library(linelist)
library(remotes)
library(janitor)
library(kableExtra)
library(DT)

```



## Load scripts

These scripts will load:

* all scripts stored as `.R` files inside `/scripts/`
* all scripts stored as `.R` files inside `/src/`

These scripts also contain routines to access the latest clean encrypted data
(see next section). 

```{r load_scripts}

reportfactory::rfh_load_scripts()

```





## Load clean data

We import the latest NHS pathways data:


```{r load_data}

x <- import_pathways() %>%
  as_tibble()
x

```



## Completion date

We extract the completion date from the NHS Pathways file timestamp:

```{r database_date}

database_date <- attr(x, "timestamp")
database_date

```

The **completion date** of the NHS Pathways data is
**`r format(database_date, format = "%A %d %b %Y")`**.




## Add variables

We add the following variable:

* `day`: an integer representing the number of days from the earliest data
reported, used for modelling purposes; the first day is 0

```{r add_variables}

x <- x %>% 
  mutate(nhs_region = str_to_title(gsub("_"," ",nhs_region)),
         nhs_region = gsub(" Of ", " of ", nhs_region),
         nhs_region = gsub(" And ", " and ", nhs_region),
         day = as.integer(date - min(date, na.rm = TRUE)))

```



## Auxiliary functions

These are functions which will be used further in the analyses.

Function to estimate the generalised R-squared as the proportion of deviance
explained by a given model:

```{r Rsq}

## Function to calculate R2 for Poisson model
## not adjusted for model complexity but all models have the same DF here

Rsq <- function(x) {
  1 - (x$deviance / x$null.deviance)
}

```

Function to extract growth rates per region as well as halving times, and the
associated 95% confidence intervals:

```{r get_r}

## function to extract the coefficients, find the level of the intercept,
## reconstruct the values of r, get confidence intervals

get_r <- function(model) {
  ##  extract coefficients and conf int
  out <- data.frame(r = coef(model))  %>%
    rownames_to_column("var") %>% 
    cbind(confint(model)) %>%
    filter(!grepl("day_of_week", var)) %>% 
    filter(grepl("day", var)) %>%
    rename(lower_95 = "2.5 %",
           upper_95 = "97.5 %") %>%
    mutate(var = sub("day:", "", var))
  
  ## reconstruct values: intercept + region-coefficient
  for (i in 2:nrow(out)) {
    out[i, -1] <- out[1, -1] + out[i, -1]
  }
  
  ## find the name of the intercept, restore regions names
  out <- out %>%
    mutate(nhs_region = model$xlevels$nhs_region) %>%
    select(nhs_region, everything(), -var)
  
  ## find halving times
  halving <- log(0.5) / out[,-1] %>%
    rename(halving_t = r,
           halving_t_lower_95 = lower_95,
           halving_t_upper_95 = upper_95)
  
  ## set halving times with exclusion intervals to NA
  no_halving <- out$lower_95 < 0 & out$upper_95 > 0
  halving[no_halving, ] <- NA_real_
  
  ## return all data
  cbind(out, halving)
  
}

```

Functions used in the correlation analysis between NHS Pathways reports and deaths:

```{r cor_functions}
## Function to calculate Pearson's correlation between deaths and lagged
## reports. Note that `pearson` can be replaced with `spearman` for rank
## correlation.

getcor <- function(x, ndx) {
  return(cor(x$deaths[ndx],
             x$note_lag[ndx],
             use = "complete.obs",
             method = "pearson"))
}

## Catch if sample size throws an error
getcor2 <- possibly(getcor, otherwise = NA)

getboot <- function(x) {
  result <- boot::boot.ci(boot::boot(x, getcor2, R = 1000), 
                           type = "bca")
  return(data.frame(n = sum(!is.na(x$note_lag) & !is.na(x$deaths)),
                    r = result$t0,
                    r_low = result$bca[4],
                    r_hi = result$bca[5]))
}



```

Function to classify the day of the week into *weekend*, *Monday*, and *the rest*:

```{r day_of_week}

## Fn to add day of week
day_of_week <- function(df) {
  df %>% 
    dplyr::mutate(day_of_week = lubridate::wday(date, label = TRUE)) %>% 
    dplyr::mutate(day_of_week = dplyr::case_when(
      day_of_week %in% c("Sat", "Sun") ~ "weekend",
      day_of_week %in% c("Mon") ~ "monday",
      !(day_of_week %in% c("Sat", "Sun", "Mon")) ~ "rest_of_week"
    ) %>% 
      factor(levels = c("rest_of_week", "monday", "weekend")))
}

```

Custom color palettes, color scales, and vectors of colors:

```{r palettes}

pal <- c("#006212",
         "#ae3cab",
         "#00db90",
         "#960c00",
         "#55aaff",
         "#ff7e78",
         "#00388d")

age.pal <- viridis::viridis(3,begin = 0.1, end = 0.7)

```





<!-- ======================================================== -->
<!-- ======================================================== -->
<!-- ======================================================== -->

# Proof of concept {.tabset .tabset-fade .tabset-pills}

## Outline

We simulate some case counts and study the behaviour of different approaches for
detecting changes in underlying rates of case incidence.



## Simulated data

We simulate the following datasets, all covering 30 days of data and using Poisson distributions:

1. **stationary**: use a constant rate 50 cases / day
2. **stationary+increase**: use a constant rate 50 for 25 days, followed by an
   exponential increase (*r* = 0.2) from day 26 onwards
3. **decrease+increase**: use an exponentially decreasing rate (*r* = -0.2) from
   a start at 50 for 25 days, followed by an exponential increase (*r* = 0.2) from day 26
   onwards

```{r simulate_data}

## Note: this is a quick and dirty simulator only used for a proof of concept

#' @param n_days number of days to run simulations for
#' @param change_at day at which to change (increase / decrease)
#' @param n_start initial number of cases to start with
#' @param r_increase daily growth rate for increase
#' @param r_decrease daily growth rate for decrease

simulate_data <- function(n_days = 30, change_at = 25, n_start = 200,
                          r_increase = 0.1, r_decrease = -0.05) {
  
  dates <- seq_len(n_days)
  n_days_change <- max(1L, n_days - change_at)
  days_change <- seq_len(n_days_change)
  
  sim_stationary <- rpois(n = n_days, lambda = n_start)

  sim_stationary_increase <- rpois(n = change_at, lambda = n_start)
  lambda_increase <- tail(sim_stationary_increase, 1L) * exp(r_increase * days_change)
  sim_stationary_increase <- c(sim_stationary_increase,
                               rpois(n = n_days_change,
                                     lambda = lambda_increase))

  days_decrease <- seq_len(change_at) - 1
  lambda_decrease <- n_start * exp(r_decrease * days_decrease)
  sim_decrease_increase <- rpois(n = change_at, lambda = lambda_decrease)
  lambda_increase <- tail(sim_decrease_increase, 1L) * exp(r_increase * days_change)
  sim_decrease_increase <- c(sim_decrease_increase,
                             rpois(n = n_days_change,
                                   lambda = lambda_increase))
  
  data.frame(dates = dates,
             stationary = sim_stationary,
             stationary_increase = sim_stationary_increase,
             decrease_increase = sim_decrease_increase)

}


x <- simulate_data()
x_long <- x %>%
  pivot_longer(-1, names_to = "simulation", values_to = "cases")

ggplot(x_long, aes(x = dates, y = cases, color = simulation)) +
  geom_point() +
  geom_line() +
  scale_color_viridis_d(end = 0.8) +
  theme_bw() +
  large_txt +
  theme(legend.position = "bottom") +
  geom_vline(xintercept = 25, lty = 2) +
  labs(title = "Simulated data")

```




## Approach 1: projection and p-value

In this approach, we:

1. a priori divide the data as:
    * first *n - k* points for fitting a model
	* last *k* points for which we assess if they correspond to a trend change
in incidence 

2. fit the model on the first *n - k* points (Poisson GLM by default)

3. derive a large number of projections from the a model for the last *k* points

4. find a metric measuring how far from the projections each of the last *k* points is


```{r approach_1}


#' Fit quasipoisson glm to data and forecast for remaining data points
#' 
#' 1. get value of lambda from its distribution
#' 2. get counts from the lambda
#'
#' @param data a `data.frame` with 2 columns: `dates`, and `counts`, in this order
#' @param k the number of last days to be left off the fitting
#' @param n_sim the number of simulations to perform
#'
#' @return a list with two components: i) a list with `k` component, each
#'   containing forecasts for the corresponding day (from the day immediately
#'   after the last day used for fitting to the last) ii) a `data.frame` of the
#'   data for the last `k` days

fit_and_forecast <- function(data, k = 1, n_sim = 1000) {

  n_fit <- max(x$dates) - k
  data_fit <- dplyr::filter(data, dates <= n_fit)
  data_test <- dplyr::filter(data, dates > n_fit)
  mod <- glm(counts ~ dates, family = "quasipoisson", data = data_fit)

  days_predict <- seq(n_fit + 1, length = k, by = 1L)
  coefs <- summary(mod)$coefficients

  intercept <- rnorm(n_sim, mean = coefs[1, 1], sd = coefs[1, 2])
  slope <- rnorm(n_sim, mean = coefs[2, 1], sd = coefs[2, 2])
  pred <- lapply(days_predict,
                 function(t)
                   rpois(n = n_sim, lambda = exp(intercept + slope * t))
                 )
  names(pred) <- days_predict

  ## return a list of predictions, and the observed data for these days
  list(pred = pred,
       obs = data_test)
  
}


pred_stationary <- x %>%
  select(dates, counts = stationary) %>%
  fit_and_forecast(k = 5)


```



<!-- =======================================================  -->
<!-- =======================================================  -->
<!-- ======================================================= -->

# System information {.tabset .tabset-fade .tabset-pills}

## Outline

The following information documents the system on which the document was
compiled.


## System 

This provides information on the operating system.

```{r system_info}
Sys.info()
```

## R environment

This provides information on the version of R used:

```{r R_session}
R.version
```


## R packages

This provides information on the packages used:

```{r R_pkg}
sessionInfo()
```

---
title: "Early detection of changes in COVID-19 incidence using NHS pathways data"
author: "Thibaut Jombart, Dirk Schumacher, Emily Nightingale, Quentin Leclerc, Sam Abbott, Michael HÃ¶hle, CMMID"
date: "`r format(Sys.time(), '%A %d %B %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: zenburn
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_collapse: no
    toc_depth: 4
    toc_float: yes
    css: !expr here::here('css', 'style.css')
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      collapse = TRUE,
                      dpi = 80,
                      fig.height = 4,
                      fig.width = 8,
                      warning = FALSE,
                      message = FALSE,
                      dev = "png")
```



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->

# Data preparation {.tabset .tabset-fade .tabset-pills}

## Outline


* **Load scripts**: loads libraries and useful scripts used in the analyses; all
`.R` files contained in `scripts` at the root of the factory are automatically
loaded

* **Load data**: imports datasets, and may contain some *ad hoc* changes to the
data such as specific data cleaning (not used in other reports), new variables
used in the analyses, etc.



## Load packages

```{r libraries}

library(reportfactory)
library(here)
library(rio) 
library(tidyverse)
library(incidence)
library(distcrete)
library(epitrix)
library(earlyR)
library(projections)
library(linelist)
library(remotes)
library(janitor)
library(kableExtra)
library(DT)
library(AnomalyDetection)
library(surveillance)
library(rsample)
library(yardstick)

```



## Load scripts

These scripts will load:

* all scripts stored as `.R` files inside `/scripts/`
* all scripts stored as `.R` files inside `/src/`

These scripts also contain routines to access the latest clean encrypted data
(see next section). 

```{r load_scripts}

reportfactory::rfh_load_scripts()

```





## Load clean data

We import the latest NHS pathways data:


```{r load_data}

pathways <- import_pathways() %>%
  as_tibble()
pathways

```



## Completion date

We extract the completion date from the NHS Pathways file timestamp:

```{r database_date}

database_date <- attr(pathways, "timestamp")
database_date

```

The **completion date** of the NHS Pathways data is
**`r format(database_date, format = "%A %d %b %Y")`**.




## Add variables

We add the following variable:

* `day`: an integer representing the number of days from the earliest data
reported, used for modelling purposes; the first day is 0

```{r add_variables}

pathways <- pathways %>% 
  mutate(nhs_region = str_to_title(gsub("_"," ",nhs_region)),
         nhs_region = gsub(" Of ", " of ", nhs_region),
         nhs_region = gsub(" And ", " and ", nhs_region),
         day = as.integer(date - min(date, na.rm = TRUE)))

```



## Auxiliary functions

These are functions which will be used further in the analyses.

Function to estimate the generalised R-squared as the proportion of deviance
explained by a given model:

```{r Rsq}

## Function to calculate R2 for Poisson model
## not adjusted for model complexity but all models have the same DF here

Rsq <- function(x) {
  1 - (x$deviance / x$null.deviance)
}

```

Function to extract growth rates per region as well as halving times, and the
associated 95% confidence intervals:

```{r get_r}

## function to extract the coefficients, find the level of the intercept,
## reconstruct the values of r, get confidence intervals

get_r <- function(model) {
  ##  extract coefficients and conf int
  out <- data.frame(r = coef(model))  %>%
    rownames_to_column("var") %>% 
    cbind(confint(model)) %>%
    filter(!grepl("day_of_week", var)) %>% 
    filter(grepl("day", var)) %>%
    rename(lower_95 = "2.5 %",
           upper_95 = "97.5 %") %>%
    mutate(var = sub("day:", "", var))
  
  ## reconstruct values: intercept + region-coefficient
  for (i in 2:nrow(out)) {
    out[i, -1] <- out[1, -1] + out[i, -1]
  }
  
  ## find the name of the intercept, restore regions names
  out <- out %>%
    mutate(nhs_region = model$xlevels$nhs_region) %>%
    select(nhs_region, everything(), -var)
  
  ## find halving times
  halving <- log(0.5) / out[,-1] %>%
    rename(halving_t = r,
           halving_t_lower_95 = lower_95,
           halving_t_upper_95 = upper_95)
  
  ## set halving times with exclusion intervals to NA
  no_halving <- out$lower_95 < 0 & out$upper_95 > 0
  halving[no_halving, ] <- NA_real_
  
  ## return all data
  cbind(out, halving)
  
}

```

Functions used in the correlation analysis between NHS Pathways reports and deaths:

```{r cor_functions}
## Function to calculate Pearson's correlation between deaths and lagged
## reports. Note that `pearson` can be replaced with `spearman` for rank
## correlation.

getcor <- function(x, ndx) {
  return(cor(x$deaths[ndx],
             x$note_lag[ndx],
             use = "complete.obs",
             method = "pearson"))
}

## Catch if sample size throws an error
getcor2 <- possibly(getcor, otherwise = NA)

getboot <- function(x) {
  result <- boot::boot.ci(boot::boot(x, getcor2, R = 1000), 
                           type = "bca")
  return(data.frame(n = sum(!is.na(x$note_lag) & !is.na(x$deaths)),
                    r = result$t0,
                    r_low = result$bca[4],
                    r_hi = result$bca[5]))
}



```

Function to classify the day of the week into *weekend*, *Monday*, and *the rest*:

```{r day_of_week}

## Fn to add day of week
day_of_week <- function(df) {
  df %>% 
    dplyr::mutate(day_of_week = lubridate::wday(date, label = TRUE)) %>% 
    dplyr::mutate(day_of_week = dplyr::case_when(
      day_of_week %in% c("Sat", "Sun") ~ "weekend",
      day_of_week %in% c("Mon") ~ "monday",
      !(day_of_week %in% c("Sat", "Sun", "Mon")) ~ "rest_of_week"
    ) %>% 
      factor(levels = c("rest_of_week", "monday", "weekend")))
}

```

Custom color palettes, color scales, and vectors of colors:

```{r palettes}

pal <- c("#006212",
         "#ae3cab",
         "#00db90",
         "#960c00",
         "#55aaff",
         "#ff7e78",
         "#00388d")

age.pal <- viridis::viridis(3,begin = 0.1, end = 0.7)

```






<!-- ======================================================== -->
<!-- ======================================================== -->
<!-- ======================================================== -->

# Simulated data {.tabset .tabset-fade .tabset-pills}

## Outline

We simulate some case counts and study the behaviour of different approaches for
detecting changes in underlying rates of case incidence.



## Simulator

We simulate the following datasets, all covering 30 days of data and using Poisson distributions:

1. **stationary**: use a constant rate 50 cases / day
2. **stationary+increase**: use a constant rate 50 for 25 days, followed by an
   exponential increase (*r* = 0.2) from day 26 onwards
3. **decrease+increase**: use an exponentially decreasing rate (*r* = -0.2) from
   a start at 50 for 25 days, followed by an exponential increase (*r* = 0.2) from day 26
   onwards

```{r simulator}

## Note: this is a quick and dirty simulator only used for a proof of concept

#' @param n_days number of days to run simulations for
#' @param change_at day at which to change (increase / decrease)
#' @param n_start initial number of cases to start with
#' @param r_increase daily growth rate for increase
#' @param r_decrease daily growth rate for decrease

simulate_data <- function(n_days = 30, change_at = 25, n_start = 300,
                          r_increase = 0.1, r_decrease = -0.075) {
  
  dates <- seq_len(n_days)
  n_days_change <- max(1L, n_days - change_at)
  days_change <- seq_len(n_days_change)
  
  sim_stationary <- rpois(n = n_days, lambda = n_start)

  sim_stationary_increase <- rpois(n = change_at, lambda = n_start)
  lambda_increase <- tail(sim_stationary_increase, 1L) * exp(r_increase * days_change)
  sim_stationary_increase <- c(sim_stationary_increase,
                               rpois(n = n_days_change,
                                     lambda = lambda_increase))

  days_decrease <- seq_len(change_at) - 1
  lambda_decrease <- n_start * exp(r_decrease * days_decrease)
  sim_decrease_increase <- rpois(n = change_at, lambda = lambda_decrease)
  lambda_increase <- tail(sim_decrease_increase, 1L) * exp(r_increase * days_change)
  sim_decrease_increase <- c(sim_decrease_increase,
                             rpois(n = n_days_change,
                                   lambda = lambda_increase))
  
  data.frame(dates = dates,
             stationary = sim_stationary,
             stationary_increase = sim_stationary_increase,
             decrease_increase = sim_decrease_increase)

}


```



## Simulated data

```{r simulated_data}

sim <- simulate_data()
sim_long <- sim %>%
  pivot_longer(-1, names_to = "simulation", values_to = "cases")

ggplot(sim_long, aes(x = dates, y = cases, color = simulation)) +
  geom_point() +
  geom_line() +
  scale_color_viridis_d(end = 0.8) +
  theme_bw() +
  large_txt +
  theme(legend.position = "bottom") +
  geom_vline(xintercept = 25, lty = 2) +
  labs(title = "Simulated data")

```





<!-- ======================================================== -->
<!-- ======================================================== -->
<!-- ======================================================== -->

# Using outlier metrics {.tabset .tabset-fade .tabset-pills}

## Implementation

In this approach, we:

1. a priori divide the data as:
    * first *n - k* points for fitting a model
	* last *k* points for which we assess if they correspond to a trend change
in incidence 

2. fit the model on the first *n - k* points (Poisson GLM by default)

3. derive a large number of projections from the a model for the different days

4. calculate several metrics measuring how far from the projections each data point is


```{r approach_1}

#' Fit quasipoisson glm to data and forecast for remaining data points
#' 
#' 1. get value of lambda from its distribution
#' 2. get counts from the lambda
#'
#' @param dates a vector of dates
#' @param counts an `integer` vector of daily incidence 
#' @param k the number of last days to be left off the fitting
#' @param n_sim the number of simulations to perform
#'
#' @return a list with two components: i) a list with `k` component, each
#'   containing forecasts for the corresponding day (from the day immediately
#'   after the last day used for fitting to the last) ii) a `data.frame` of the
#'   data for the last `k` days

fit_and_forecast <- function(dates, counts, k = 1, n_sim = 1000) {

  n <- max(dates, na.rm = TRUE)
  n_fit <- n - k
  days <- seq_along(dates)
  
  data <- data.frame(dates = dates, counts = counts)
  data_fit <- dplyr::filter(data, dates <= n_fit)
  data_test <- dplyr::filter(data, dates > n_fit)

  mod <- glm(counts ~ dates, family = "quasipoisson", data = data_fit)

  days_predict <- seq(n_fit + 1, length = k, by = 1L)
  coefs <- summary(mod)$coefficients

  intercept <- rnorm(n_sim, mean = coefs[1, 1], sd = coefs[1, 2])
  slope <- rnorm(n_sim, mean = coefs[2, 1], sd = coefs[2, 2])
  pred <- lapply(days,
                 function(t)
                   rpois(n = n_sim, lambda = exp(intercept + slope * t))
                 )
  names(pred) <- days

  ## return a list of predictions, and the observed data for these days
  list(pred = pred,
       obs = data)
  
}



## wrapper for all the above:
## i) fits a model and derives predictions
## ii) calculates outlier stats for each data point

get_stats <- function(dates, counts, k = 1, n_sim = 1000, C = 2) {
  
  ## OUTLIER STATS
  ## Note: these are custom stats considered for the current problem; more are
  ## listed on https://en.wikipedia.org/wiki/Outlier; several make parametric
  ## asumptions


  ## Standardised residual
  ## 
  ## Note that the prediction is approximated as the mean of the empirical
  ## distribution. We report the absolute value of the standardised residual.

  get_resid <- function(obs, sim) {
    abs(obs - mean(sim, na.rm = TRUE)) / sd(sim, na.rm = TRUE)
  }


  ## 'IQRD' index (made-up)
  ##
  ## Stands for Inter-Quartile Range Distance. This index measures the absolute
  ## distance from the median in IQR units; in other words, how many IQR away from
  ## the median is the observation?

  get_iqrd <- function(obs, sim) {
    abs(obs - median(sim, na.rm = TRUE)) / IQR(sim, na.rm = TRUE)
  }


  ## MAD deviation
  ##
  ## This one already exists and relies on the Median Absolute Deviation (MAD)
  ## to measure existing variation in the data.

  get_mad <- function(obs, sim) {
    abs(obs - median(sim, na.rm = TRUE)) / mad(sim, na.rm = TRUE)
  }


  ## Get model results, and apply statistics to them
  res <- fit_and_forecast(dates, counts, k = k, n_sim = n_sim)

  n <- length(dates)

  stats <- list(
    resid = numeric(n),
    iqrd = numeric(n),
    mad = numeric(n)
  )

  ## calculate the different statistics
  for (i in seq_len(n)) {
    stats$resid[i] <- get_resid(obs = res$obs$counts[i], sim = res$pred[[i]])
    stats$iqrd[i] <- get_iqrd(obs = res$obs$counts[i], sim = res$pred[[i]])
    stats$mad[i] <- get_mad(obs = res$obs$counts[i], sim = res$pred[[i]])
  }


  ## average metric (mean of other metrics after rescaling on 0-1)
  rescale <- function(x) (x - min(x, na.rm = TRUE)) / max(x, na.rm = TRUE)

  stats$combined <- rowMeans(data.frame(stats))

  ## define outliers a function of the median of each stat, so that x_i is an
  ## outlier if (x_i > C * median)
  n_fit <- n - k
  days_fit <- seq_len(n_fit)
  thresholds <- sapply(stats,
                       function(e) median(e[days_fit], na.rm = TRUE)) * C
  outliers <- lapply(seq_along(stats),
                     function(i)
                       stats[[i]] >= thresholds[i])
  names(outliers) <- paste0("outlier_", names(stats))  
  
  ## assemble results
  stats <- data.frame(stats)
  names(stats) <- paste0("stat_", names(stats))
  out <- cbind.data.frame(dates = dates,
                          counts = counts,
                          stats,
                          outliers)

  out
}


```




## Results: stationary

```{r results_stationary}

## stationary

res_stationary <-  get_stats(sim$dates, sim$stationary)

res_stationary_long_stats <- res_stationary %>%
  select(1:2, contains("stat")) %>%
  pivot_longer(-(1:2), names_to = "metric", values_to = "stat") %>%
  mutate(metric = gsub("stat_", "", metric))

## res_stationary_long_outliers <- res_stationary %>%
##   select(1:2, contains("outlier")) %>%
##   pivot_longer(-(1:2), names_to = "metric", values_to = "outlier") %>%
##   mutate(metric = gsub("outlier_", "", metric))

## res_stationary_long <- full_join(res_stationary_long_stats,
##                                  res_stationary_long_outliers)

res_stationary_long_stats %>%
  ggplot(aes(x = dates, y = stat, color = metric)) +
  facet_wrap( ~ metric, scales = "free") +
  geom_vline(xintercept = 25, linetype = 2) +
  theme_bw() +
  large_txt +
  geom_point() +
  geom_line() +
  guides(color = FALSE)

```




## Results: stationary + increase

```{r}

res_stationary_increase <- get_stats(sim$dates, sim$stationary_increase)

res_stationary_increase_long_stats <- res_stationary_increase %>%
  select(1:2, contains("stat")) %>%
  pivot_longer(-(1:2), names_to = "metric", values_to = "stat") %>%
  mutate(metric = gsub("stat_", "", metric))

res_stationary_increase_long_stats %>%
  ggplot(aes(x = dates, y = stat, color = metric)) +
  facet_wrap( ~ metric, scales = "free") +
  geom_vline(xintercept = 25, linetype = 2) +
  theme_bw() +
  large_txt +
  geom_point() +
  geom_line() +
  guides(color = FALSE)
 
```


## Results: decrease + increase

```{r}

res_decrease_increase <- get_stats(sim$dates, sim$decrease_increase)

res_decrease_increase_long_stats <- res_decrease_increase %>%
  select(1:2, contains("stat")) %>%
  pivot_longer(-(1:2), names_to = "metric", values_to = "stat") %>%
  mutate(metric = gsub("stat_", "", metric))

res_decrease_increase_long_stats %>%
  ggplot(aes(x = dates, y = stat, color = metric)) +
  facet_wrap( ~ metric, scales = "free") +
  geom_vline(xintercept = 25, linetype = 2) +
  theme_bw() +
  large_txt +
  geom_point() +
  geom_line() +
  guides(color = FALSE)

```





<!-- ======================================================== -->
<!-- ======================================================== -->
<!-- ======================================================== -->

# Model selection and outliers {.tabset .tabset-fade .tabset-pills}

## Outline

This approach does the following:

* splits the data into a training set ('older' data points) and an test set
  containing the *k* most recent data points, for which we evaluate if they
  represent trend shifts

* fit a number of models to the training, evaluate each using cross-validation,
  and retains the best-predicting model
  
* derive the x-th quantiles of the predictive distribution of the model

* classify each point outside these quantiles as 'outliers'


  

## Implementation

This code is largely contributed by Dirk Schumacher in his
[dirky-hack](https://gist.github.com/dirkschumacher/dce2b83276a180375d24eb89b1334ef4). 

In the absence of seasonality, the models retained include:

* constant Poisson model
* Poisson with day effect
* Poisson with day and weekday effect
* NegBin with day effect
* NebBin with day and weekday effect


Models:

```{r models}

.validate_data <- function(data, vars = c("count")) {
  if (is.null(data)) stop("`data` is NULL")
  if (!is.data.frame(data)) stop("`data` is not a `data.frame`")
  if (!nrow(data)) stop("`data` has 0 rows")
  for (e in vars) {
    if (!e %in% names(data)) {
      msg <- sprintf("missing column in `data`: %s", e)
      stop(msg)
    }
  }
}


## poisson models
model_poisson_constant <- function(data) {
  .validate_data(data)
  glm(data = data, formula = count ~ 1, family = "poisson")
}

model_poisson_time <- function(data) {
  .validate_data(data, c("count", "day"))
  glm(data = data, formula = count ~ 1 + day, family = "poisson")
}

model_poisson_time_weekday <- function(data) {
  .validate_data(data, c("count", "day", "weekday"))
  glm(data = data, formula = cases ~ 1 + day + weekday, family = "poisson")
}


## negative binomial models
model_nb_time <- function(data) {
  .validate_data(data, c("count", "day"))
  MASS::glm.nb(data = data, formula = count ~ 1 + day)
}

model_nb_time_weekday <- function(data) {
  .validate_data(data, c("count", "day", "weekday"))
  MASS::glm.nb(data = data, formula = cases ~ 1 + day + weekday)
}


## quasi-poisson models
model_qp_time <- function(data) {
  .validate_data(data, c("count", "day"))
  glm(data = data, formula = count ~ 1 + day, family = "quasipoisson")
}

model_qp_time_weekday <- function(data) {
  .validate_data(data, c("count", "day", "weekday"))
  glm(data = data, formula = cases ~ 1 + day + weekday, family = "quasipoisson")
}


## make a list of all models tested
models <- list(
  list(name = "poisson_constant", model = model_poisson_constant),
  list(name = "poisson_time", model = model_poisson_time),
  # list(name = "poisson_time_weekday", model = model_poisson_time_weekday)
  list(name = "nb_time", model = model_nb_time)
  # list(name = "nb_time_weekday", model = model_nb_time_weekday)
)

```


Functions to obtain RMSE for Jackniffed samples:

```{r rmse_functions}

## auxiliary function to fit a model on training set and get RMSE on testing
## set
get_rmse <- function(data, id_test, model) {
  data_fit <- data[-id_test, ,drop = FALSE]
  data_test <- data[id_test, ,drop = FALSE]
  fitted_model <- model(data_fit)
  predictions <- predict(fitted_model,
                         newdata = data_test,
                         type = "response")
  sqrt(sum(data_test$count - predictions)^2)
}


## applies the function above for all jacknife samples and returns the median
## of all RMSE; see ?rsample::vfold_cv for more options

jack_rmse <- function(data, model) {
  n <- nrow(data)
  all_rmse <- vapply(seq_len(n),
                     function(i) get_rmse(data, id_test = i, model = model$model),
                     numeric(1))

  median(all_rmse, na.rm = TRUE)
}

```

Functions to select 'best' model:

```{r select_model}

select_model <- function(x, models) {

  models_mrse_values <- vapply(models,
                        function(model) jack_rmse(x, model),
                        numeric(1))
  models_names <- vapply(models,
                         function(e) e$name,
                         character(1))
  models_results <- data.frame(models_names,
                               models_mrse_values)

  ## return best model
  out <- list(mrse = models_results,
              model = models[[which.min(models_mrse_values)]])
  out
  
}


```


Function to find outliers:

```{r find_outliers}

## Function to find outlier in supplementary observations
find_outliers <- function(fitted_model, newdata, alpha = 0.05) {
  pred_ci <- ciTools::add_pi(tb = newdata, fit = fitted_model,
                             alpha = alpha, names = c("lower", "upper"))
  dplyr::mutate(pred_ci,
                outlier = count < lower | count > upper,
                classif = case_when(
                  count < lower ~ "decrease",
                  count > upper ~ "increase",
                  TRUE ~ "normal"),
                classif = factor(classif,
                                 levels = c("increase", "normal", "decrease")))
}

```


General wrapper:

```{r detect_epichange}

detect_epichange <- function(x, models, day = 1, count = 2,
                             k = 1, alpha = 0.05) {
  
  x <- data.frame(day = x[, day],
                  count = x[, count])
  
  n <- nrow(x)
  x_train <- head(x, n - k)
  x_test <- tail(x, k)

  models_comparison <- select_model(x_train, models)
  best_model <- models_comparison$model
  fit <- best_model$model(x_train)
  diagnostics <- find_outliers(fitted_model = fit, x, alpha = alpha)

  diagnostics <- mutate(diagnostics,
                        type = dplyr::if_else(1:n() <= (n - k),
                                              "train",
                                              "test"))
  out <- list(all_models = models,
              models_mrse = models_comparison$mrse,
              best_model = best_model,
              model_fit = fit,
              k = k,
              results = diagnostics)
  class(out) <- c("epichange", class(out))
  out
}

```


Plotting method:

```{r plot.epichange}

plot.epichange <- function(x,
                           col_normal = "#8B8B8C",
                           col_increase = "#CB3355",
                           col_decrease = "#32AB96",
                           ...) {

  n_train <- nrow(x$results) - x$k

  scale_classif <- scale_color_manual(
    "Change in trend:",
    values = c(decrease = col_decrease, increase = col_increase, normal = col_normal),
    labels = c(decrease = "Decrease", increase = "Increase", normal = "Same trend"),
    drop = FALSE)

  
  ggplot(x$results, aes(x = day, y = count)) +
    theme_bw() +
    large_txt +
    geom_vline(xintercept = n_train, linetype = 2) +
    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
    geom_point(aes(color = classif), size = 3 + x$results$outlier) +
    geom_line(alpha = 0.5) +
    geom_line(aes(y = pred)) +
    scale_classif +
    theme(legend.position = "bottom") +
    guides(color = guide_legend(override.aes = list(size = c(4, 4, 3))))
}

```



## Results on simple simulations


```{r results_sim}

res_stationary <- detect_epichange(sim, count = "stationary", models, k = 5, alpha = 0.05)
res_stationary
plot(res_stationary)


res_stationary_increase <- detect_epichange(sim, count = "stationary_increase", models, k = 5, alpha = 0.05)
res_stationary_increase
plot(res_stationary_increase)

res_decrease_increase <- detect_epichange(sim, count = "decrease_increase", models, k = 5, alpha = 0.05)
res_decrease_increase
plot(res_decrease_increase)

```




<!-- ======================================================== -->
<!-- ======================================================== -->
<!-- ======================================================== -->

# Farrington algorithm {.tabset .tabset-fade .tabset-pills}


## Results

It seems `sts` has not been designed with daily counts in mind; so far the
following generates an error. Will get back to it later.

```{r farrington, eval = FALSE}

sim_sts <- sts(observed = sim$stationary, epoch = sim$dates, frequency = 365)
farringtonFlexible(sim_stationary_sts) # generates an error


```






<!-- ======================================================== -->
<!-- ======================================================== -->
<!-- ======================================================== -->

# Using Seasonal Hybrid ESD {.tabset .tabset-fade .tabset-pills}

## Results: stationary

```{r }

AnomalyDetectionVec(sim$stationary, period = 7, plot = TRUE, direction = "both", max_anoms = 0.05)

```



## Results: stationary + increase

```{r }

AnomalyDetectionVec(sim$stationary_increase, period = 7, plot = TRUE, direction = "both", max_anoms = 0.05)

```



## Results: decrease + increase

```{r }

AnomalyDetectionVec(sim$decrease_increase, period = 7, plot = TRUE, direction = "both", max_anoms = 0.05)

```



## Other approaches

* a nice overview of R packages for [changepoint analysis](https://lindeloev.github.io/mcp/articles/packages.html)

* [GUI](https://blog.exploratory.io/introduction-to-anomaly-detection-in-r-with-exploratory-a0507d40385d)
  for anomaly detection amongst other things

* Package [AnomalyDetection](https://github.com/twitter/AnomalyDetection)
  implementing the Seasonal Hybrid ESD (S-H-ESD) algorithm developed by twitter






<!-- =======================================================  -->
<!-- =======================================================  -->
<!-- ======================================================= -->

# System information {.tabset .tabset-fade .tabset-pills}

## Outline

The following information documents the system on which the document was
compiled.


## System 

This provides information on the operating system.

```{r system_info}
Sys.info()
```

## R environment

This provides information on the version of R used:

```{r R_session}
R.version
```


## R packages

This provides information on the packages used:

```{r R_pkg}
sessionInfo()
```
